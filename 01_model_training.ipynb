{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2434a683",
   "metadata": {},
   "source": [
    "# PokerML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcefa6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable some console warnings\n",
    "import os\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af043357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30c0167",
   "metadata": {},
   "source": [
    "Make sure that you have Vivado suite in your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c375ab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PATH'] = os.environ['XILINX_VIVADO'] + '/bin:' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b44f731",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86514582",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data\"\n",
    "\n",
    "MODELS_DIR = 'models/'\n",
    "DATA_NPY_DIR = DATA_DIR + '/npy/'\n",
    "\n",
    "IMAGE_WIDTH = 64\n",
    "IMAGE_HEIGHT = 64\n",
    "TEST_SIZE = 0.2\n",
    "NUM_CLASSES = 52\n",
    "\n",
    "QKERAS_TRAIN = False ## I did not debug yet the issue with the QKeras saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf0e75c",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240b2617",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val = np.load(DATA_NPY_DIR + 'X_train_val.npy')\n",
    "X_test = np.load(DATA_NPY_DIR + 'X_test.npy')\n",
    "y_train_val = np.load(DATA_NPY_DIR + 'y_train_val.npy')\n",
    "y_test = np.load(DATA_NPY_DIR + 'y_test.npy')\n",
    "classes = np.load(DATA_NPY_DIR + 'classes.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ed16bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train and validation set:', X_train_val.shape[0])\n",
    "print('Test set:                ', X_test.shape[0])\n",
    "print('Classes:                 ', classes.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c78db2",
   "metadata": {},
   "source": [
    "## Train QKeras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6a2f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE, CLEAN THESE IMPORTS, MESSY\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "#from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.layers import Activation\n",
    "from qkeras import QDense, QActivation, QConv2D\n",
    "from qkeras.quantizers import quantized_bits, quantized_relu\n",
    "\n",
    "# # ENABLE THIS IF YOU USE PRUNING\n",
    "# from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
    "# from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "# import tensorflow_model_optimization as tfmot\n",
    "\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e5c241",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CHECKPOINT_FILENAME = MODELS_DIR + 'qkeras/model.h5'\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(QConv2D(4,\n",
    "                 kernel_size=(3, 3),\n",
    "                 input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3),\n",
    "                 kernel_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "                 bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "                 kernel_initializer='lecun_uniform',\n",
    "                 kernel_regularizer=l1(0.0001),\n",
    "                 name='conv1'))\n",
    "\n",
    "model.add(QActivation(activation=quantized_relu(6),\n",
    "                      name='relu1'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),\n",
    "                       name='maxpool1'))\n",
    "\n",
    "model.add(QConv2D(8,\n",
    "                 kernel_size=(3, 3),\n",
    "                 kernel_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "                 bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "                 kernel_initializer='lecun_uniform',\n",
    "                 kernel_regularizer=l1(0.0001),\n",
    "                 name='conv2'))\n",
    "\n",
    "model.add(QActivation(activation=quantized_relu(6),\n",
    "                      name='relu2'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),\n",
    "                       name='maxpool2'))\n",
    "\n",
    "model.add(Flatten(name='flatten'))\n",
    "\n",
    "model.add(QDense(32,\n",
    "                 kernel_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "                 bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "                 kernel_initializer='lecun_uniform',\n",
    "                 kernel_regularizer=l1(0.0001),\n",
    "                 name='dense1'))\n",
    "\n",
    "model.add(QActivation(activation=quantized_relu(6),\n",
    "                      name='relu3'))\n",
    "\n",
    "model.add(QDense(NUM_CLASSES,\n",
    "                 kernel_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "                 bias_quantizer=quantized_bits(6, 0, alpha=1),\n",
    "                 kernel_initializer='lecun_uniform',\n",
    "                 kernel_regularizer=l1(0.0001),\n",
    "                 activation='softmax',\n",
    "                 name='output'))\n",
    "\n",
    "# # adding pruning \n",
    "# pruning_params = {\n",
    "#     'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "#         initial_sparsity=0.50,\n",
    "#         final_sparsity=0.80,\n",
    "#         begin_step=200,\n",
    "#         end_step=1000)\n",
    "# }\n",
    "# model = prune.prune_low_magnitude(model, **pruning_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cb8752",
   "metadata": {},
   "outputs": [],
   "source": [
    "if QKERAS_TRAIN:\n",
    "    from qkeras.utils import model_save_quantized_weights\n",
    "\n",
    "    # Using learning rate with exponential decay\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        0.001,\n",
    "        decay_steps=100000,\n",
    "        decay_rate=0.96,\n",
    "        staircase=True\n",
    "    )\n",
    "    adam = Adam(learning_rate=lr_schedule)\n",
    "    \n",
    "    #adam = Adam(learning_rate=0.001)\n",
    "\n",
    "    model.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "\n",
    "    # A few other callbacks. These should not give warnings on deprecated features.\n",
    "    callbacks = [\n",
    "            ModelCheckpoint(\n",
    "                CHECKPOINT_FILENAME,\n",
    "                monitor='val_loss',\n",
    "                verbose=0,\n",
    "                save_best_only=True,\n",
    "                save_weights_only=False,\n",
    "                save_freq='epoch')\n",
    "       #,\n",
    "       #pruning_callbacks.UpdatePruningStep()\n",
    "       #,#ReduceLROnPlateau(patience=75, min_delta=1**-6), \n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_val,\n",
    "        y_train_val,\n",
    "        batch_size=128,\n",
    "        epochs=20,\n",
    "        validation_split=0.25,\n",
    "        shuffle=True,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "# # Strip the model of pruning information\n",
    "#     model = strip_pruning(model)\n",
    "#    model.save(CHECKPOINT_FILENAME)\n",
    "\n",
    "    # Use this instead if you use the callbacks\n",
    "    history_file = CHECKPOINT_FILENAME.replace('.h5', '-history.pkl')\n",
    "    with open(history_file, 'wb') as file_pi:\n",
    "        pickle.dump(history.history, file_pi)\n",
    "\n",
    "    print(f'Saving history to: {history_file}')\n",
    "    print(f'Saved checkpoint to: {CHECKPOINT_FILENAME}')\n",
    "\n",
    "\n",
    "else:\n",
    "    from tensorflow.keras.models import load_model\n",
    "    from qkeras.utils import _add_supported_quantized_objects\n",
    "    co = {}\n",
    "    _add_supported_quantized_objects(co)\n",
    "    model = load_model(CHECKPOINT_FILENAME, custom_objects=co, compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0644b187",
   "metadata": {},
   "outputs": [],
   "source": [
    "if QKERAS_TRAIN:\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['accuracy'], label='Train')\n",
    "    plt.plot(history.history['val_accuracy'], label='Test')\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'], label='Train')\n",
    "    plt.plot(history.history['val_loss'], label='Test')\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12dd892",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_qkeras = model.predict(np.ascontiguousarray(X_test))\n",
    "\n",
    "print(\"QKeras accuracy: {:.6f}%\".format(100.*accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_qkeras, axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6ab571",
   "metadata": {},
   "source": [
    "Pre trained model sul have accuracy = 98.030708%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4935b80",
   "metadata": {},
   "source": [
    "## QKeras to hls4ml (Quantization Aware Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ea201b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hls4ml\n",
    "import plotting\n",
    "\n",
    "# First, the baseline model\n",
    "hls_config = hls4ml.utils.config_from_keras_model(model, granularity='name')\n",
    "\n",
    "# Set the precision and reuse factor for the full model\n",
    "hls_config['Model']['Precision'] = 'ap_fixed<32,16>'\n",
    "hls_config['Model']['ReuseFactor'] = 128\n",
    "\n",
    "# Create an entry for each layer, here you can for instance change the strategy for a layer to 'resource'\n",
    "# or increase the reuse factor individually for large layers.\n",
    "for Layer in hls_config['LayerName'].keys():\n",
    "    hls_config['LayerName'][Layer]['Strategy'] = 'Resource'\n",
    "    hls_config['LayerName'][Layer]['ReuseFactor'] = 200\n",
    "    hls_config['LayerName'][Layer]['Trace'] = True\n",
    "\n",
    "hls_config['LayerName']['output']['Strategy'] = 'Stable'\n",
    "plotting.print_dict(hls_config)\n",
    "\n",
    "cfg = hls4ml.converters.create_config(backend='VivadoAccelerator')\n",
    "cfg['IOType'] = 'io_stream'  # Must set this if using CNNs!\n",
    "cfg['HLSConfig'] = hls_config\n",
    "cfg['KerasModel'] = model\n",
    "cfg['OutputDir'] = 'projects/qat_hls4ml_prj'\n",
    "#cfg['XilinxPart'] = 'xcu250-figd2104-2L-e'\n",
    "\n",
    "hls_model = hls4ml.converters.keras_to_hls(cfg)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "69780387",
   "metadata": {},
   "source": [
    "# DISABLE THIS CELL\n",
    "import hls4ml\n",
    "import plotting\n",
    "\n",
    "config = hls4ml.utils.config_from_keras_model(model, granularity='name')\n",
    "config['LayerName']['output']['exp_table_t'] = 'ap_fixed<18,8>'\n",
    "config['LayerName']['output']['inv_table_t'] = 'ap_fixed<18,4>'\n",
    "print(\"-----------------------------------\")\n",
    "plotting.print_dict(config)\n",
    "print(\"-----------------------------------\")\n",
    "\n",
    "config['Model']['Precision'] = 'ap_fixed<16,6>'\n",
    "config['Model']['ReuseFactor'] = 256\n",
    "\n",
    "hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "    model, hls_config=config, output_dir='projects/qat_hls4ml_prj', part='xcu250-figd2104-2L-e'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b3048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade5610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF PREDICTION TAKE TOO MUCH PLEASE\n",
    "# REDUCE THE NUMBER OF INPUTS\n",
    "print(X_test.shape)\n",
    "print(X_test[:10].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8aac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_qkeras = model.predict(np.ascontiguousarray(X_test))\n",
    "y_hls = hls_model.predict(np.ascontiguousarray(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8602e617",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "#print(\"Accuracy baseline:  {:.6f}%\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_keras, axis=1))))\n",
    "print(\"Accuracy quantized: {:.6f}%\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_qkeras, axis=1))))\n",
    "print(\"Accuracy hls4ml:    {:.6f}%\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_hls, axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f2f77a",
   "metadata": {},
   "source": [
    "```\n",
    "With pretrained model you should expect:\n",
    "Accuracy quantized: 0.980307%\n",
    "Accuracy hls4ml:    0.979973%\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9936d809",
   "metadata": {},
   "source": [
    "# THIS AND THE FOLLOWING CELL CAN BE USED TO TUNE THE BITWIDTH OF THE LAYERS\n",
    "# THEY TAKE A LOT OF TIME TO RUN SO FOR THE TIME BEING I DISABLE THEM\n",
    "# IF YOU WANT TO MAKE accuracy hls4 == accuracy qkeras WE WILL USE IT\n",
    "_, hls_trace = hls_model.trace(np.ascontiguousarray(X_test)) \n",
    "keras_trace = hls4ml.model.profiling.get_ymodel_keras(model, X_test) \n",
    "\n",
    "print(f'HLS Keys: {hls_trace.keys()}')\n",
    "print(f'Keras Keys: {keras_trace.keys()}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e5360660",
   "metadata": {},
   "source": [
    "layers = ['conv1', 'relu1', 'maxpool1', 'conv2', 'relu2', 'maxpool2', 'dense1', 'relu3', 'output_softmax']\n",
    "\n",
    "for idx, layer in enumerate(layers):\n",
    "    keras_layer, hls_layer = keras_trace[layer], hls_trace[layer]\n",
    "    try:\n",
    "        diff = np.average(np.abs(keras_layer - hls_layer ))\n",
    "        print(f'{layer}', '\\t\\t', diff)\n",
    "        \n",
    "        plt.figure(figsize=(7, 5))\n",
    "\n",
    "        plt.scatter(hls_layer.flatten(), keras_layer.flatten())\n",
    "        min_x = min(keras_layer.min(), hls_layer.min())\n",
    "        max_x = min(keras_layer.max(), hls_layer.max())\n",
    "\n",
    "        onnx_min, onnx_max = keras_layer.flatten().min(), keras_layer.flatten().max()\n",
    "        hls_min, hls_max = hls_layer.flatten().min(), hls_layer.flatten().max()\n",
    "        \n",
    "        print(f'hls/keras min: {hls_min}/{onnx_min}')\n",
    "        print(f'hls/keras max: {hls_max}/{onnx_max}')\n",
    "        \n",
    "        plt.plot([min_x, max_x], [min_x, max_x], c='red')\n",
    "        plt.axhline(min_x, c='red')\n",
    "        plt.axhline(max_x, c='red')\n",
    "\n",
    "        plt.title(f'(hls) {layer} -- (keras) {layer}')\n",
    "        plt.xlabel(f'hls4ml - [{hls_min:.3f},  {hls_max:.3f}]')\n",
    "        plt.ylabel(f'keras - [{onnx_min:.3f},  {onnx_max:.3f}]')\n",
    "        plt.yscale('linear')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7717b24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(cm, class_names):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix using seaborn's heatmap.\n",
    "    \n",
    "    Args:\n",
    "        cm (array, shape = [n, n]): Confusion matrix\n",
    "        class_names (array, shape = [n]): Array of class names\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 9))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "y_test_ = np.argmax(y_test, axis=1)\n",
    "y_qkeras_ = np.argmax(y_qkeras, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_test_, y_qkeras_)\n",
    "\n",
    "plot_confusion_matrix(cm, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83204546",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_model.build(\n",
    "    csim=False,\n",
    "    synth=False,\n",
    "    cosim=False,\n",
    "    export=False,\n",
    "    vsynth=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc126ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls4ml.report.read_vivado_report('projects/qat_hls4ml_prj')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "361.267px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
